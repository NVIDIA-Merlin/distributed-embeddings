


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>embedding &mdash; distributed-embeddings  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/jquery.fancybox.min.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/glpi.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="distributed-embeddings  documentation" href="index.html"/>
        <link rel="next" title="dist_model_parallel" href="dist_model_parallel.html"/>
        <link rel="prev" title="Distributed Model Parallel" href="userguide.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> distributed-embeddings
          

          
          </a>

          
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Distributed Embeddings</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="userguide.html">Distributed Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="userguide.html#embedding-layers">Embedding Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="userguide.html#input-hashing">Input Hashing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">embedding</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#embedding-layers">Embedding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1"><span class="hidden-section">Embedding</span></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#distributed_embeddings.python.layers.embedding.Embedding"><code class="docutils literal notranslate"><span class="pre">Embedding</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#embedding-ops">Embedding Ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#embedding-lookup"><span class="hidden-section">embedding_lookup</span></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#distributed_embeddings.python.ops.embedding_lookup_ops.embedding_lookup"><code class="docutils literal notranslate"><span class="pre">embedding_lookup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#integerlookup-layers">IntegerLookup Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2"><span class="hidden-section">Embedding</span></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#distributed_embeddings.python.layers.embedding.IntegerLookup"><code class="docutils literal notranslate"><span class="pre">IntegerLookup</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dist_model_parallel.html">dist_model_parallel</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">distributed-embeddings</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>embedding</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/embedding.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="dist_model_parallel.html" class="btn btn-neutral float-right" title="dist_model_parallel" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="userguide.html" class="btn btn-neutral" title="Distributed Model Parallel" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="module-distributed_embeddings.python">
<span id="embedding"></span><h1>embedding<a class="headerlink" href="#module-distributed_embeddings.python" title="Permalink to this heading">¶</a></h1>
<section id="embedding-layers">
<h2>Embedding Layers<a class="headerlink" href="#embedding-layers" title="Permalink to this heading">¶</a></h2>
<section id="id1">
<h3><span class="hidden-section">Embedding</span><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.Embedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">distributed_embeddings.python.layers.embedding.</span></span><span class="sig-name descname"><span class="pre">Embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/distributed_embeddings/python/layers/embedding.html#Embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Turns indices into vectors of fixed size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – Size of the vocabulary, i.e. maximum index + 1.</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – Length of embedding vectors.</p></li>
<li><p><strong>embeddings_initializer</strong> – Initializer for the <cite>embeddings</cite>
matrix (see <cite>keras.initializers</cite>).</p></li>
<li><p><strong>embeddings_regularizer</strong> – Regularizer function applied to
the <cite>embeddings</cite> matrix (see <cite>keras.regularizers</cite>).</p></li>
<li><p><strong>embeddings_constraint</strong> – Constraint function applied to
the <cite>embeddings</cite> matrix (see <cite>keras.constraints</cite>).</p></li>
<li><p><strong>combiner</strong> (<em>str</em>) – Reduction method, [‘sum’, ‘mean’] or None. Default None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>When combiner is not None, supported input and their respectively output shape are:</dt><dd><p>N-D <cite>Tensor</cite>: <cite>(d1,…,dn)</cite>, output shape: <cite>(d1,…,dn-1,output_dim)</cite>, N &gt;= 2
2-D <cite>RaggedTensor</cite>: <cite>(batch_size, ragged_dim)</cite>, output shape: <cite>(batch_size, output_dim)</cite>
2-D <cite>SparseTensor</cite>: <cite>(batch_size, max_hotness)</cite>, output shape: <cite>(batch_size, output_dim)</cite></p>
</dd>
</dl>
<p>Embedding picked from last input dimension will be reduced with given combiner.</p>
<dl class="py method">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.Embedding.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.Embedding.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call. It is invoked automatically before
the first execution of <cite>call()</cite>.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses
(at the discretion of the subclass implementer).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_shape</strong> – Instance of <cite>TensorShape</cite>, or list of instances of
<cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.Embedding.compute_output_shape">
<span class="sig-name descname"><span class="pre">compute_output_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.Embedding.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>This method will cause the layer’s state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input_shape</strong> – Shape tuple (tuple of integers) or <cite>tf.TensorShape</cite>,
or structure of shape tuples / <cite>tf.TensorShape</cite> instances
(one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <cite>tf.TensorShape</cite> instance
or structure of <cite>tf.TensorShape</cite> instances.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.Embedding.from_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/distributed_embeddings/python/layers/embedding.html#Embedding.from_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.Embedding.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a layer from its config.
Overriding this to enable instatiating fast embedding from keras embedding configs</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.Embedding.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/distributed_embeddings/python/layers/embedding.html#Embedding.get_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.Embedding.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<p>Note that <cite>get_config()</cite> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="embedding-ops">
<h2>Embedding Ops<a class="headerlink" href="#embedding-ops" title="Permalink to this heading">¶</a></h2>
<section id="embedding-lookup">
<h3><span class="hidden-section">embedding_lookup</span><a class="headerlink" href="#embedding-lookup" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="distributed_embeddings.python.ops.embedding_lookup_ops.embedding_lookup">
<span class="sig-prename descclassname"><span class="pre">distributed_embeddings.python.ops.embedding_lookup_ops.</span></span><span class="sig-name descname"><span class="pre">embedding_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combiner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/distributed_embeddings/python/ops/embedding_lookup_ops.html#embedding_lookup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#distributed_embeddings.python.ops.embedding_lookup_ops.embedding_lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Looks up embeddings for the given <cite>ids</cite> from a embedding tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param</strong> (<em>Tensor</em>) – A single tensor representing the complete embedding tensor.</p></li>
<li><p><strong>ids</strong> (<em>Tensor</em>) – A 2D <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite> containing the ids to be looked up
in <cite>param</cite>. Also support <cite>RaggedTensor</cite> and <cite>SparseTensor</cite>.</p></li>
<li><p><strong>combiner</strong> (<em>string or None</em>) – Reduction method, [‘sum’, ‘mean’] or None. Default None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Tensor</em> – A <cite>Tensor</cite> with the same type as the tensors in <cite>param</cite>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When combiner is None, returned tensor has shape: <code class="docutils literal notranslate"><span class="pre">shape(ids)</span> <span class="pre">+</span> <span class="pre">shape(param)[1]</span></code></p>
<p>Otherwise, embedding from same row is reduced and returned tensor has shape:
<code class="docutils literal notranslate"><span class="pre">shape(ids)[0]</span> <span class="pre">+</span> <span class="pre">shape(param)[1]</span></code></p>
</div>
<p>Note when ids is RaggedTensor, its values and row_splits are col_index and row_index
of CSR format hotness matrix, thus can be directly constructed.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> – If <cite>param</cite> is empty.</p></li>
<li><p><strong>ValueError</strong> – If <cite>ids</cite> is not 2D tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="integerlookup-layers">
<h2>IntegerLookup Layers<a class="headerlink" href="#integerlookup-layers" title="Permalink to this heading">¶</a></h2>
<section id="id2">
<h3><span class="hidden-section">Embedding</span><a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="distributed_embeddings.python.layers.embedding.IntegerLookup">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">distributed_embeddings.python.layers.embedding.</span></span><span class="sig-name descname"><span class="pre">IntegerLookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/distributed_embeddings/python/layers/embedding.html#IntegerLookup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#distributed_embeddings.python.layers.embedding.IntegerLookup" title="Permalink to this definition">¶</a></dt>
<dd><p>A preprocessing layer which maps integer features to contiguous ranges.
Vocabulary is generated on the fly, static vocabulary and adapt() will be supported.
Partially support features of tf.keras.layers.IntegerLookup.
Frequency of keys are counted when GPU algorithm is used.</p>
</dd></dl>

</section>
</section>
</section>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dist_model_parallel.html" class="btn btn-neutral float-right" title="dist_model_parallel" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="userguide.html" class="btn btn-neutral" title="Distributed Model Parallel" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/documentation_options.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="_static/jquery.fancybox.min.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
  <script type="text/javascript">
    $(function(){
      $('.image-reference').fancybox();
    })
  </script>

</body>
</html>