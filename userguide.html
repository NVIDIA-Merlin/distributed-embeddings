


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Model Parallel &mdash; distributed-embeddings  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/jquery.fancybox.min.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/glpi.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="distributed-embeddings  documentation" href="index.html"/>
        <link rel="next" title="embedding" href="embedding.html"/>
        <link rel="prev" title="Distributed Embeddings" href="Introduction.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> distributed-embeddings
          

          
          </a>

          
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Distributed Embeddings</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Model Parallel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#automatic-model-parallel-wrapper">Automatic model parallel wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="#large-table-exceeding-single-gpu-s-memory">Large table exceeding single GPU’s memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mix-matching-distribution-strategies">Mix Matching Distribution Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shared-embedding">Shared Embedding</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#embedding-layers">Embedding Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="#input-hashing">Input Hashing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_model_parallel.html">dist_model_parallel</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">distributed-embeddings</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Distributed Model Parallel</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/userguide.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="embedding.html" class="btn btn-neutral float-right" title="embedding" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction.html" class="btn btn-neutral" title="Distributed Embeddings" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="distributed-model-parallel">
<h1>Distributed Model Parallel<a class="headerlink" href="#distributed-model-parallel" title="Permalink to this heading">¶</a></h1>
<section id="automatic-model-parallel-wrapper">
<h2>Automatic model parallel wrapper<a class="headerlink" href="#automatic-model-parallel-wrapper" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">distributed_embeddings.dist_model_parallel</span></code> enables model parallel
training by changing only three lines of your script. It can also be
used alongside data parallel to form hybrid parallel training.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dist_model_parallel</span> <span class="k">as</span> <span class="nn">dmp</span>

<span class="k">class</span> <span class="nc">MyEmbeddingModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span>  <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="o">*</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">table_sizes</span><span class="p">]</span>
    <span class="c1"># add this line to wrap list of embedding layers used in the model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layers</span> <span class="o">=</span> <span class="n">dmp</span><span class="o">.</span><span class="n">DistributedEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_layers</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># embedding_outputs = [e(i) for e, i in zip(self.embedding_layers, inputs)]</span>
    <span class="n">embedding_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layers</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>To work with Horovod data parallel, replace Horovod <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> and
broadcast. Take following example directly from Horovod
<a class="reference external" href="https://horovod.readthedocs.io/en/stable/tensorflow.html">documents</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">first_batch</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

  <span class="c1"># Change Horovod Gradient Tape to dmp tape</span>
  <span class="c1"># tape = hvd.DistributedGradientTape(tape)</span>
  <span class="n">tape</span> <span class="o">=</span> <span class="n">dmp</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">)</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
  <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
    <span class="c1"># Change Horovod broadcast_variables to dmp&#39;s</span>
    <span class="c1"># hvd.broadcast_variables(model.variables, root_rank=0)</span>
    <span class="n">dmp</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">loss_value</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">dist_model_parallel</span></code> can be applied on distributed-embeddings embedding layers,
keras embedding layers and any user defined custom embedding layer.</p>
</section>
<section id="large-table-exceeding-single-gpu-s-memory">
<h2>Large table exceeding single GPU’s memory<a class="headerlink" href="#large-table-exceeding-single-gpu-s-memory" title="Permalink to this heading">¶</a></h2>
<p>If single embedding table exceeds GPU memory, or portion of GPU memory considering
optimizer’s memory requirement, we have to split the embedding table and distribute
them to multiple GPU.
Currently distributed-embeddings supports column slicing embedding tables by passing
<code class="docutils literal notranslate"><span class="pre">column_slice_threshold</span></code> to DistributedEmbedding, Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split embedding tables that are larger than 200000 elements (not Bytes)</span>
<span class="n">embedding_layers</span> <span class="o">=</span> <span class="n">dmp</span><span class="o">.</span><span class="n">DistributedEmbedding</span><span class="p">(</span><span class="n">embedding_layers</span><span class="p">,</span> <span class="n">column_slice_threshold</span><span class="o">=</span><span class="mi">200000</span><span class="p">)</span>
</pre></div>
</div>
<p>Embedding will be evenly split into the smallest power of 2 number of slices so that each
slice is smaller than <code class="docutils literal notranslate"><span class="pre">column_slice_threshold</span></code>.</p>
<p>Alternatively, user can specify <code class="docutils literal notranslate"><span class="pre">row_slice_threshold</span></code>, example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split embedding tables that are larger than 200000 elements (not Bytes)</span>
<span class="n">embedding_layers</span> <span class="o">=</span> <span class="n">dmp</span><span class="o">.</span><span class="n">DistributedEmbedding</span><span class="p">(</span><span class="n">embedding_layers</span><span class="p">,</span> <span class="n">row_slice_threshold</span><span class="o">=</span><span class="mi">200000</span><span class="p">)</span>
</pre></div>
</div>
<p>Different from <code class="docutils literal notranslate"><span class="pre">column_slice_threshold</span></code>, table larger than the threshold will be sliced
on the first dimension into ‘rows’. Another difference is that table will be sliced evenly
into the num_worker slices and get distributed among all workers.
This is useful when user have super tall and narrow table.</p>
</section>
<section id="mix-matching-distribution-strategies">
<h2>Mix Matching Distribution Strategies<a class="headerlink" href="#mix-matching-distribution-strategies" title="Permalink to this heading">¶</a></h2>
<p>DistributedEmbedding have flexible api options allowing user specify how they want embedding layers to be distributed among GPUs. When multiple options are used, strategies will be applied in following order, balancing ease of use and fine grained control:</p>
<p><code class="docutils literal notranslate"><span class="pre">data_parallel_threshold</span></code> - Table smaller than the threshold will run in data parallel. This greatly reduces communication in case of small tables with large amount of lookups.</p>
<p><code class="docutils literal notranslate"><span class="pre">row_slice_threshold</span></code> - Table with more elements than it will be sliced into rows and distributed evenly onto all workers.</p>
<p><code class="docutils literal notranslate"><span class="pre">column_slice_threshold</span></code> - This is the most flexible option. Tables that aren’t running in dp or row slice will get here and get sliced into columns smaller than column_slice_threshold.</p>
<p>We currently don’t support partial participation on data parallel and row slice. So tables under those strategies will be distributed onto all workers. For the rest of tables, some may have been column sliced, one of the following strategies will apply to distribute them with model parallel:</p>
<p><code class="docutils literal notranslate"><span class="pre">basic</span></code> - round-robin distribute table slices in original order</p>
<p><code class="docutils literal notranslate"><span class="pre">memory_balanced</span></code> - round-robin distribute table slices by size order. This mode balances compute and memory.</p>
<p><code class="docutils literal notranslate"><span class="pre">memory_optimized</span></code> - distribute table slices to achieve most even memory usage. This mode helps avoid OOM in workloads with skewed tables sizes.</p>
<p><strong>In summary:</strong></p>
<ol class="arabic simple">
<li><p>Small tables run data parallel on all workers</p></li>
<li><p>Largest tables get evenly row slied onto all workers</p></li>
<li><p>All other tables run in model parallel, potentially after 2-way to max workers way column slice</p></li>
</ol>
</section>
<section id="shared-embedding">
<h2>Shared Embedding<a class="headerlink" href="#shared-embedding" title="Permalink to this heading">¶</a></h2>
<p>It is common that some features share embedding. For example, watched video and browsed video can
share video embedding. User can supports this case by passing <code class="docutils literal notranslate"><span class="pre">input_table_map</span></code> at intialization
time, example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The first and the last input both map to embedding 0</span>
<span class="n">embedding_layers</span> <span class="o">=</span> <span class="n">dmp</span><span class="o">.</span><span class="n">DistributedEmbedding</span><span class="p">(</span>
    <span class="n">embedding_layers</span><span class="p">,</span>
    <span class="n">input_table_map</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="embedding-layers">
<h1>Embedding Layers<a class="headerlink" href="#embedding-layers" title="Permalink to this heading">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">distributed_embeddings.Embedding</span></code> combines functionalities of
<code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Embedding</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup_sparse</span></code>
under a unified Keras layer API. The backend is designed to achieve high
GPU efficiency. Two kinds of inputs are supported. We call them
fixed/variable hotness as opposite to confusing dense/sparse term
various TF API uses. The difference is whether all sample in the batch
contains same number of indices.
Fixed hotness inputs are regular <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> while variable hotness
inputs are 2D <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> or <code class="docutils literal notranslate"><span class="pre">SparseTensor</span></code>. Elements of inputs are
ids to be looked up. Lookup output from inner most dimension are considered
from same sample and will be reduced if combiner is used.</p>
<p>Examples:</p>
<p><strong>One-hot embedding:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">onehot_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">onehot_input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 1, 64)</span>
</pre></div>
</div>
<p><strong>Fixed hotness embedding:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fixedhot_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">fixedhot_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 7)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">fixedhot_input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 7, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">combiner</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">fixedhot_input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 64)</span>
</pre></div>
</div>
<p><strong>Variable hotness embedding:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vhot_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">87</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">929</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">vhot_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(5, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">vhot_input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(5, None, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">combiner</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">vhot_input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(5, 64)</span>
</pre></div>
</div>
</section>
<section id="input-hashing">
<h1>Input Hashing<a class="headerlink" href="#input-hashing" title="Permalink to this heading">¶</a></h1>
<p>A preprocessing layer that maps integer features to contiguous ranges. This layer extends <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.IntegerLookup</span></code> with following functionalities:</p>
<ol class="arabic simple">
<li><p>Generates vocabulary on the fly so that training can start with empty vocabulary</p></li>
<li><p>Suport both CPU and GPU with efficient backends</p></li>
<li><p>Frequency of input keys are counted when GPU backend is used</p></li>
<li><p>Overflow protection. When lookup table grows beyond user-defined limit, new keys will be treat as OOV tokens and get mapped to 0.</p></li>
</ol>
<p>With this, user can start or continugous train on new data, without offline data preprocessing.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lookup_layer</span> <span class="o">=</span> <span class="n">IntegerLookup</span><span class="p">(</span><span class="n">max_vocab_size</span><span class="p">)</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_vocab_size</span><span class="p">,</span> <span class="n">embedding_width</span><span class="p">)</span>
<span class="o">...</span>
<span class="c1"># inside call() function</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">lookup_layer</span><span class="p">(</span><span class="n">input_hash_keys</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</pre></div>
</div>
<p>For more details, see our Criteo <a class="reference external" href="https://github.com/NVIDIA-Merlin/distributed-embeddings/blob/main/examples/criteo/main.py">Example</a>
and read TensorFlow <a class="reference external" href="https://www.tensorflow.org/guide/migrate/migrating_feature_columns">Preprocessing Layer Document</a></p>
</section>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="embedding.html" class="btn btn-neutral float-right" title="embedding" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction.html" class="btn btn-neutral" title="Distributed Embeddings" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/documentation_options.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="_static/jquery.fancybox.min.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
  <script type="text/javascript">
    $(function(){
      $('.image-reference').fancybox();
    })
  </script>

</body>
</html>